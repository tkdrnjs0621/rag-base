{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import argparse\n",
    "import re\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from functools import partial\n",
    "import logging\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.retriever import Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: facebook/contriever-msmarco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkdrnjs0621/miniconda3/envs/torch_241/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing passages from files ['data/corpus/wikipedia_embeddings/passages_00', 'data/corpus/wikipedia_embeddings/passages_01', 'data/corpus/wikipedia_embeddings/passages_02', 'data/corpus/wikipedia_embeddings/passages_03', 'data/corpus/wikipedia_embeddings/passages_04', 'data/corpus/wikipedia_embeddings/passages_05', 'data/corpus/wikipedia_embeddings/passages_06', 'data/corpus/wikipedia_embeddings/passages_07', 'data/corpus/wikipedia_embeddings/passages_08', 'data/corpus/wikipedia_embeddings/passages_09', 'data/corpus/wikipedia_embeddings/passages_10', 'data/corpus/wikipedia_embeddings/passages_11', 'data/corpus/wikipedia_embeddings/passages_12', 'data/corpus/wikipedia_embeddings/passages_13', 'data/corpus/wikipedia_embeddings/passages_14', 'data/corpus/wikipedia_embeddings/passages_15']\n",
      "Loading file data/corpus/wikipedia_embeddings/passages_00\n",
      "Total data indexed 1000000\n",
      "Loading file data/corpus/wikipedia_embeddings/passages_01\n",
      "Total data indexed 2000000\n",
      "Loading file data/corpus/wikipedia_embeddings/passages_02\n",
      "Total data indexed 3000000\n",
      "Loading file data/corpus/wikipedia_embeddings/passages_03\n",
      "Total data indexed 4000000\n",
      "Total data indexed 5000000\n",
      "Loading file data/corpus/wikipedia_embeddings/passages_04\n",
      "Total data indexed 6000000\n",
      "Loading file data/corpus/wikipedia_embeddings/passages_05\n",
      "Total data indexed 7000000\n",
      "Loading file data/corpus/wikipedia_embeddings/passages_06\n",
      "Total data indexed 8000000\n",
      "Total data indexed 9000000\n",
      "Loading file data/corpus/wikipedia_embeddings/passages_07\n",
      "Total data indexed 10000000\n",
      "Loading file data/corpus/wikipedia_embeddings/passages_08\n",
      "Total data indexed 11000000\n",
      "Loading file data/corpus/wikipedia_embeddings/passages_09\n",
      "Total data indexed 12000000\n",
      "Total data indexed 13000000\n",
      "Loading file data/corpus/wikipedia_embeddings/passages_10\n",
      "Total data indexed 14000000\n",
      "Loading file data/corpus/wikipedia_embeddings/passages_11\n",
      "Total data indexed 15000000\n",
      "Loading file data/corpus/wikipedia_embeddings/passages_12\n",
      "Total data indexed 16000000\n",
      "Total data indexed 17000000\n",
      "Loading file data/corpus/wikipedia_embeddings/passages_13\n",
      "Total data indexed 18000000\n",
      "Loading file data/corpus/wikipedia_embeddings/passages_14\n",
      "Total data indexed 19000000\n",
      "Loading file data/corpus/wikipedia_embeddings/passages_15\n",
      "Total data indexed 20000000\n",
      "Total data indexed 21000000\n",
      "Total data indexed 21015324\n",
      "Data indexing completed.\n",
      "Indexing time: 227.0 s.\n",
      "loading passages\n",
      "passages have been loaded\n"
     ]
    }
   ],
   "source": [
    "from src.retriever import Retriever\n",
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "arg = SimpleNamespace()\n",
    "arg.retrieval_model_name_or_path=\"facebook/contriever-msmarco\"\n",
    "arg.retrieval_embedding_size=768\n",
    "arg.passages='data/corpus/psgs_w100.tsv'\n",
    "arg.passages_embeddings='data/corpus/wikipedia_embeddings/*'\n",
    "arg.indexing_batch_size=1000000\n",
    "arg.save_or_load_index = False\n",
    "arg.retrieval_n_subquantizers=0\n",
    "arg.retrieval_n_bits=8\n",
    "arg.max_k=100\n",
    "arg.lowercase = False\n",
    "arg.normalize_text = False\n",
    "arg.per_gpu_batch_size=1000000\n",
    "arg.question_maxlength=100000\n",
    "\n",
    "retriever = Retriever(arg)\n",
    "retriever.setup_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a35c6fb79f50482398610c4012773ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ICTNLP/Auto-RAG-Llama-3-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"ICTNLP/Auto-RAG-Llama-3-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c45139a24974580aa536e2628045145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"ICTNLP/Auto-RAG-Llama-3-8B-Instruct\",device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_message = [\n",
    "    {'role':'system','content':\"\"\"Answer the question by retrieving external knowledge. \n",
    "Extract useful information from each retrieved document. \n",
    "If the information is insufficient or irrelevant, \n",
    "refine your query and search again until you are able to answer the question.\"\"\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1:  who is known as the father of the hydrogen bomb?\n",
      "Questions embeddings shape: torch.Size([1, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.82s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search time: 10.8 s.\n",
      "Query 2:  who is commonly referred to as the father of the hydrogen bomb?\n",
      "Questions embeddings shape: torch.Size([1, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.44s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search time: 6.4 s.\n",
      "===========\n",
      "###system\n",
      " Answer the question by retrieving external knowledge. \n",
      "Extract useful information from each retrieved document. \n",
      "If the information is insufficient or irrelevant, \n",
      "refine your query and search again until you are able to answer the question.\n",
      "###user\n",
      "Question: who is called the father of hydrogen bomb\n",
      "###assistant\n",
      "Analysis: To answer this question, I would need to know who is commonly referred to as the \"Father of the Hydrogen Bomb\".\n",
      "\n",
      "Initial Query: Who is known as the father of the hydrogen bomb?\n",
      "###user\n",
      "Retrieved Document_1: John Clive Ward has been said that physicists have made use of his principles and developments \"often without knowing it, and generally without quoting him.\" In 1955, Ward was recruited to work at the Atomic Weapons Research Establishment at Aldermaston. There, he independently derived a version of the Teller-Ulam design, for which he has been called the \"father of the British H-bomb\". John Clive Ward was born in East Ham, London, on 1 August 1924. He was the son of Joseph William Ward, a civil servant who worked in Inland Revenue, and his wife Winifred Palmer, a schoolteacher. He had a sister, Mary\n",
      "Edward Teller a role in this). In 1952 he left Los Alamos and joined the newly established Livermore branch of the University of California Radiation Laboratory, which had been created largely through his urging. After the detonation of Ivy Mike, the first thermonuclear weapon to utilize the Teller–Ulam configuration, on November 1, 1952, Teller became known in the press as the \"father of the hydrogen bomb.\" Teller himself refrained from attending the test—he claimed not to feel welcome at the Pacific Proving Grounds—and instead saw its results on a seismograph in the basement of a hall in Berkeley. There was an opinion\n",
      "###assistant\n",
      "Intermediate Answer_1: Based on the Retrieved Document_1, John Clive Ward is referred to as the \"father of the British H-bomb\". However, the question asks who is called the father of the hydrogen bomb.\n",
      "\n",
      "Refined Query: Who is commonly referred to as the father of the hydrogen bomb?\n",
      "###user\n",
      "Retrieved Document_2: John Clive Ward has been said that physicists have made use of his principles and developments \"often without knowing it, and generally without quoting him.\" In 1955, Ward was recruited to work at the Atomic Weapons Research Establishment at Aldermaston. There, he independently derived a version of the Teller-Ulam design, for which he has been called the \"father of the British H-bomb\". John Clive Ward was born in East Ham, London, on 1 August 1924. He was the son of Joseph William Ward, a civil servant who worked in Inland Revenue, and his wife Winifred Palmer, a schoolteacher. He had a sister, Mary\n",
      "Stanislaw Ulam This balanced view is shared by others who were involved, including Mark and Fermi, but Teller persistently attempted to downplay Ulam's role. \"After the H-bomb was made,\" Bethe recalled, \"reporters started to call Teller the father of the H-bomb. For the sake of history, I think it is more precise to say that Ulam is the father, because he provided the seed, and Teller is the mother, because he remained with the child. As for me, I guess I am the midwife.\" With the basic fusion reactions confirmed, and with a feasible design in hand, there was nothing to prevent\n",
      "###assistant\n",
      "Intermediate Answer_2: Based on the Retrieved Document_2, Edward Teller is commonly referred to as the father of the hydrogen bomb.\n",
      "\n",
      "Final Answer: Edward Teller.\n",
      "-----------\n",
      " edward teller.\n"
     ]
    }
   ],
   "source": [
    "question = \"who is called the father of hydrogen bomb\"\n",
    "message = base_message.copy()\n",
    "\n",
    "message.append({'role':'user','content':\"Question: \"+question})\n",
    "n_iter=1\n",
    "answer=''\n",
    "while(n_iter<6):\n",
    "    result_m = pipe(message)[0]['generated_text']\n",
    "    result = result_m[-1]['content']\n",
    "    if('query:' in result.lower()):\n",
    "        iq = result.lower().split('query:')[-1]\n",
    "        print(f'Query {n_iter}:',iq)\n",
    "        retrieved = retriever.search_document([iq],2)\n",
    "        txt = '\\n'.join([t['title']+' '+t['text'] for t in retrieved[0]])\n",
    "        message = result_m\n",
    "        message.append({'role':'user','content':f\"Retrieved Document_{n_iter}: {txt.strip()}\"})\n",
    "        n_iter+=1\n",
    "    elif 'final answer' in result.lower():\n",
    "        message = result_m\n",
    "        answer = result.lower().split('final answer:')[-1]\n",
    "        break\n",
    "    else:\n",
    "        print('error')\n",
    "        break\n",
    "print('===========')\n",
    "for t in message:\n",
    "    print('###'+t['role'])\n",
    "    print(t['content'])\n",
    "print('-----------')\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_241",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
